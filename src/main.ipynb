{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab732094",
   "metadata": {},
   "source": [
    "# Proyecto 2 - Procesamiento de lenguaje natural\n",
    "## CommonLit - Evaluate Student Summaries\n",
    "\n",
    "**Francis Aguilar - 22243**<br>\n",
    "**Diego García - 22404**<br>\n",
    "**César López - 22535**<br>\n",
    "**Ángela García - 22869**\n",
    "\n",
    "\n",
    "Link del github:<br>\n",
    "https://github.com/DiegoGarV/Proyecto2-DS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f25f98b",
   "metadata": {},
   "source": [
    "## Planteamiento inicial\n",
    "\n",
    "1. Situación problemática:<br> \n",
    "En el ámbito educativo, la evaluación de resúmenes escritos por estudiantes suele ser un proceso manual, demandante y sujeto a la subjetividad del evaluador. Esto genera dificultades para proporcionar retroalimentación oportuna y consistente, especialmente en contextos con gran número de alumnos. El reto de CommonLit plantea la necesidad de desarrollar métodos automáticos de procesamiento de lenguaje natural (PLN) que apoyen la evaluación objetiva de los resúmenes en cuanto a contenido y calidad de redacción.\n",
    "\n",
    "2. Problema científico:<br>\n",
    "¿Es posible construir un modelo de PLN que, a partir del texto de un resumen y su prompt asociado, prediga con precisión las calificaciones de contenido y redacción, utilizando métricas cuantitativas como el MCRMSE para validar su desempeño?\n",
    "\n",
    "3. Objetivo general:<br>\n",
    "Desarrollar y evaluar un modelo de procesamiento de lenguaje natural capaz de predecir automáticamente las calificaciones de contenido y redacción en resúmenes estudiantiles, con el fin de apoyar la evaluación educativa de manera objetiva y eficiente.\n",
    "\n",
    "4. Objetivos específicos:<br>\n",
    "    i. Analizar el dataset de CommonLit para explorar las características lingüísticas y distribuciones de las calificaciones asociadas a los resúmenes.<br>\n",
    "    ii. Implementar un sistema de representación textual y entrenar modelos de regresión basados en TF-IDF y algoritmos de aprendizaje supervisado.<br>\n",
    "    iii. Validar el desempeño de los modelos utilizando validación cruzada y MCRMSE, comparando los resultados obtenidos con diferentes enfoques de modelado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2767c412",
   "metadata": {},
   "source": [
    "## Investigación preliminar\n",
    "\n",
    "Las técnicas más comunes para la detección de patrones en procesamiento de lenguaje natural incluyen métodos estadísticos, representaciones vectoriales y modelos de aprendizaje automático. Entre las que más pueden aportar al proyecto son:\n",
    "\n",
    "1. Análisis de frecuencia y n-gramas: permiten identificar qué palabras o combinaciones de palabras aparecen con mayor regularidad en los textos, revelando estructuras recurrentes y patrones de estilo.\n",
    "\n",
    "2. Vectorización de texto (TF-IDF, embeddings): transforman el lenguaje en representaciones numéricas que capturan la importancia relativa de los términos o sus relaciones semánticas.\n",
    "\n",
    "3. Modelos de clasificación o regresión supervisada: como regresión lineal, máquinas de soporte vectorial o redes neuronales, que aprenden patrones complejos para predecir etiquetas o puntajes a partir de los textos.\n",
    "\n",
    "Estas técnicas pueden ayudar a extraer características del lenguaje en los resumenes estudiantiles y relacionarlos con las calificaciones de contenido y redacción. Los primeros dos ayudarán a facilitar la captura del vocabulario más característico de los resumenes y el último ayudará a asociar los patrones con el puntaje. De esta manera, se obtiene un sistema capaz de generalizar y evaluar automáticamente nuevos textos de manera objetiva y consistente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dcbd96",
   "metadata": {},
   "source": [
    "## Análisis inicial del problema y los datos disponibles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670bfbe4",
   "metadata": {},
   "source": [
    "#### Descripción de la data\n",
    "\n",
    "**summaries_train:**\n",
    "- student_id: muestra el identificador único del estudiante.\n",
    "- prompt_id: muestra el identificador único de la instrucción que se le dió al estudiante para hacer su resumen.\n",
    "- text: el resumen hecho por el estudiante\n",
    "- content: valor númerico continuo que representa la evaluación del resumen en que tan bien logra simplificar el texto el estudiante. Se toma en cuenta la coherencia, neutralidad, lógica y objetividad en el resumen.\n",
    "- wording: valor numérico continuo que representa la evaluación del resumen en cuanto a la presentación de las ideas. Se enfoca en la redacción y el uso del lenguaje, más allá del contenido.\n",
    "\n",
    "**prompts_train:**\n",
    "- prompt_id: muestra el identificador único de la instrucción que se le dió al estudiante para hacer su resumen.\n",
    "- prompt_title: título de la instrucción utilizada.\n",
    "- prompt_question: la pregunta o instrucción específica que se le hizo al estudiante para poder generar su resumen.\n",
    "- prompt_text: el texto completo que se debía resumir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e0c18",
   "metadata": {},
   "source": [
    "#### Consideraciones\n",
    "\n",
    "La data muestra tanto textos como resumenes en inglés. Existen más de 7200 resumenes diferentes, pero están divididos en 4 textos(prompts). Es importante tener en cuenta, la vairación de longitud y el estilo en los resumenes. Además, en ellos puede existir faltas ortográficas que se tomarán como el ruido del dataset.\n",
    "\n",
    "La escala original de content y wording debe ser preservada. Esto permitirá que el MCRMSE (promedio de RMSE) sea interpretable. Por lo tanto, no se normalizarán estos valores o los resultados de las metricas no darán la información que se busca.\n",
    "\n",
    "Es importante destacar que la momento de entrenar al modelo, se debe de tomar en cuenta el prompt id tanto para separar los casos por pregunta, como para entender como diferentes preguntas pueden llevar a diferentes resultados de resumen. La diferencia en tamaño y complejidad entre prompts puede llegar a confundir al modelo si se llega a ignorar. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775b52bc",
   "metadata": {},
   "source": [
    "#### Justificación de las técnicas de detección de patrones\n",
    "\n",
    "**1. Análisis de frecuencia y n-gramas:**\n",
    " - Otorga una representación numérica, dispersa y eficiente que pondera términos discriminativos.\n",
    " - Ayudará a detectar marcadores de calidad como conectores, variedad léxica, etc. Además, permite tener una mayor idea de que palabras pueden asociarse a un mayor contenido o estilo.\n",
    " - Se debe de tener cuidado con el sobreajuste de los n-gramas, pues esto puede causar que solo se enfoque en un solo prompt. Para ello se harán validaciones por grupos y se limitaran las features.\n",
    "\n",
    "**2. Vectorización con TF-IDF:**\n",
    " - Captura patrones léxicos básicos y compuestos.\n",
    " - Sus mayores beneficios serán el fuerte baseline en tareas de evaluación de texto con poco costo computacional. Además, permite integrar los n-gramas para capturar micro patrones de estilo y contenido.\n",
    " - Se debe tomar en cuenta que esta técnica no capta semántica profunda ni relaciones de largo alcance.\n",
    "\n",
    "**3. Modelos supervisados:**\n",
    " - Permite mapear la representación TF-IDF a predicciones continuas con un sesgo y varianza controlados. \n",
    " - Cuenta con un entrenamiento rápido y reproducible. Sus coeficientes permiten entender los resultados facilmente. Funciona bien con datos dispersos.\n",
    " - Se debe de tomar en cuenta los resultados del baseline para evitar errores si la relación entre el estilo y el contenido es no lineal."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
